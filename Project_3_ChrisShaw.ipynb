{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dmarx/psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the PushshiftApi\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#telling it when to start looking for posts\n",
    "start_epoch=int(dt.datetime(2017, 1, 1).timestamp())\n",
    "\n",
    "#taking posts from\n",
    "politics = list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit='politics',\n",
    "                            filter=['url','author', 'title', 'subreddit'],\n",
    "                            limit=30000))\n",
    "\n",
    "scifi = list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit='scifi',\n",
    "                            filter=['url','author', 'title', 'subreddit'],\n",
    "                            limit=30000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting the scrapes into dataframes\n",
    "politics = pd.DataFrame(politics)\n",
    "scifi    = pd.DataFrame(scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving them to csv\n",
    "politics.to_csv('./politics_scrape')\n",
    "scifi.to_csv('./scifi_scrape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = pd.read_csv('./politics_scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Youâ€™ll Never Believe It, but the Shutdown Is M...\n",
       "1    Rand Paul headed to Canada for surgery, but wi...\n",
       "2    Tiffany Trump Not Fazed Dating Someone Who Gre...\n",
       "3    Why the power elite continues to dominate Amer...\n",
       "4    Sen. Bernie Sanders says 'we will end' Big Pha...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics.title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi = pd.read_csv('./scifi_scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        New to Sci-Fi\n",
       "1    Pushing Ice by Al Reynolds and Seveneves by Ne...\n",
       "2      Any \"Top lists\" in this sub like in r/fantasy ?\n",
       "3    You can watch Star Trek discovery with Kingon ...\n",
       "4    An open-source rocket could reshape society in...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scifi.title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining them into one dataframe\n",
    "df = pd.DataFrame.append(politics, scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicates\n",
    "df = df[df.duplicated(subset='title',keep='first')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unncessary columns\n",
    "df.drop(columns=['url','author','d_','created_utc','created','Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarizing the subreddit column so 1 is politics and 0 is science\n",
    "df['subreddit'] = np.where(df['subreddit'] == 'politics', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_titles (title):\n",
    "    \n",
    "    # remove non-letters        \n",
    "    just_letters = re.sub(\"[^a-zA-Z]\", \" \", title)\n",
    "\n",
    "    #make lowercase\n",
    "    lower_letters = just_letters.lower()\n",
    "    \n",
    "    return  lower_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_titles = []\n",
    "\n",
    "for i in df['title']:\n",
    "    clean_titles.append(cleaning_titles(i))\n",
    "\n",
    "df['title'] = clean_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>you ll never believe it  but the shutdown is m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rand paul headed to canada for surgery  but wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>tiffany trump not fazed dating someone who gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>why the power elite continues to dominate amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sen  bernie sanders says  we will end  big pha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title\n",
       "0          1  you ll never believe it  but the shutdown is m...\n",
       "1          1  rand paul headed to canada for surgery  but wi...\n",
       "2          1  tiffany trump not fazed dating someone who gre...\n",
       "3          1  why the power elite continues to dominate amer...\n",
       "4          1  sen  bernie sanders says  we will end  big pha..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Data and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y variables\n",
    "X    = df.drop(columns='subreddit',axis=1)\n",
    "y    = df.drop(columns='title',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40964, 1)\n",
      "(40964, 1)\n"
     ]
    }
   ],
   "source": [
    "#checking shape\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size    = .3,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y,\n",
    "                                                    shuffle      = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing count vectorizer\n",
    "vect = CountVectorizer(stop_words='english',\n",
    "                      ngram_range=(1,10),\n",
    "                      max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vect.fit_transform(X_train['title'])\n",
    "X_test_vect  = vect.transform(X_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28674, 5000)\n",
      "(12290, 5000)\n",
      "(28674, 1)\n",
      "(12290, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vect.shape)\n",
    "print(X_test_vect.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#importing models and metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression w/ Count Vectorizer Train score: 0.980923484689963\n",
      "Logistic Regression w/ Count Vectorizer Test score: 0.9613506916192026\n",
      "Number of features: 5000\n",
      "\n",
      "Random Forest w/ Count Vectorizer Train score: 0.8470391295250053\n",
      "Random Forest w/ Count Vectorizer Test score: 0.8397070789259561\n",
      "Number of features: 5000\n"
     ]
    }
   ],
   "source": [
    "#fitting random forest\n",
    "rf = RandomForestClassifier(n_estimators    = 50,\n",
    "                            max_features    = 500,\n",
    "                            max_depth       = 30, \n",
    "                            n_jobs          = -1)\n",
    "\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "\n",
    "#fitting logistic regression\n",
    "\n",
    "log = LogisticRegression(random_state = 42)\n",
    "model = log.fit(X_train_vect, y_train)\n",
    "\n",
    "print('Logistic Regression w/ Count Vectorizer Train score:',        model.score(X_train_vect, y_train))\n",
    "print('Logistic Regression w/ Count Vectorizer Test score:' ,        model.score(X_test_vect,  y_test))\n",
    "print('Number of features:', X_train_vect.shape[1] )\n",
    "print('')\n",
    "print('Random Forest w/ Count Vectorizer Train score:',        rf_model.score(X_train_vect, y_train))\n",
    "print('Random Forest w/ Count Vectorizer Test score:' ,        rf_model.score(X_test_vect,  y_test))\n",
    "print('Number of features:', X_train_vect.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression w/ TFIDF Train score: 0.976250261560996\n",
      "Logistic Regression w/ TFIDF Test score: 0.9624084621643613\n",
      "Number of features: 5000\n",
      "\n",
      "Random Forest w/ TFIDF Train score: 0.8592104345400013\n",
      "Random Forest w/ TFIDF Test score: 0.8523189585028479\n",
      "Number of features: 5000\n"
     ]
    }
   ],
   "source": [
    "#instantiating TFIDF\n",
    "vect_tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,10),max_features=5000)\n",
    "\n",
    "X_train_vect_tfidf = vect_tfidf.fit_transform(X_train['title'])\n",
    "X_test_vect_tfidf  = vect_tfidf.transform(X_test['title'])\n",
    "\n",
    "#fitting to random forest\n",
    "rf = RandomForestClassifier(n_estimators    = 50,\n",
    "                            max_features    = 200,\n",
    "                            max_depth       = 30, \n",
    "                            n_jobs          = -1)\n",
    "rf.fit(X_train_vect_tfidf, y_train)\n",
    "\n",
    "#fitting to logistic regression\n",
    "log = LogisticRegression(random_state = 42)\n",
    "model = log.fit(X_train_vect_tfidf, y_train)\n",
    "\n",
    "print('Logistic Regression w/ TFIDF Train score:',        model.score(X_train_vect_tfidf, y_train))\n",
    "print('Logistic Regression w/ TFIDF Test score:' ,        model.score(X_test_vect_tfidf,  y_test))\n",
    "print('Number of features:', X_train_vect_tfidf.shape[1] )\n",
    "print('')\n",
    "print('Random Forest w/ TFIDF Train score:',        rf.score(X_train_vect_tfidf,y_train))\n",
    "print('Random Forest w/ TFIDF Test score:',         rf.score(X_test_vect_tfidf,y_test))\n",
    "print('Number of features:', X_train_vect_tfidf.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21074187305325398, 'trump'),\n",
       " (0.053298702449732956, 'sci fi'),\n",
       " (0.05243591851048748, 'sci'),\n",
       " (0.04753360836923412, 'fi'),\n",
       " (0.03410735932958589, 'scifi'),\n",
       " (0.03146243585317323, 'sps'),\n",
       " (0.03076854205762591, 'shutdown'),\n",
       " (0.027191348855218837, 'space'),\n",
       " (0.026672902793294314, 'star'),\n",
       " (0.018856504899946377, 'fiction')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most valuable features from TFIDF\n",
    "sorted(list(zip(rf.feature_importances_,vect_tfidf.get_feature_names())),reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression w/ Hash Vectorizer Train score: 0.9364581153658367\n",
      "Logistic Regression w/ Hash Vectorizer Test score: 0.9006509357200977\n",
      "Number of features: 5000\n",
      "\n",
      "Random Forest w/ Hash Vectorizer Train score: 0.8587570621468926\n",
      "Random Forest w/ Hash Vectorizer Test score: 0.8304312449145647\n",
      "Number of features: 5000\n"
     ]
    }
   ],
   "source": [
    "#instantiating hashing vectorizer\n",
    "vect_hash = HashingVectorizer(stop_words='english',ngram_range=(1,10),n_features=5000)\n",
    "\n",
    "X_train_vect_hash = vect_hash.fit_transform(X_train['title'])\n",
    "X_test_vect_hash = vect_hash.transform(X_test['title'])\n",
    "\n",
    "#fitting to random forest\n",
    "rf = RandomForestClassifier(n_estimators    = 50,\n",
    "                            max_features    = 200,\n",
    "                            max_depth       = 30, \n",
    "                            n_jobs          = -1)\n",
    "rf.fit(X_train_vect_hash, y_train)\n",
    "\n",
    "#fitting to logistic regression\n",
    "log = LogisticRegression(random_state = 42)\n",
    "model = log.fit(X_train_vect_hash, y_train)\n",
    "\n",
    "print('Logistic Regression w/ Hash Vectorizer Train score:',        model.score(X_train_vect_hash, y_train))\n",
    "print('Logistic Regression w/ Hash Vectorizer Test score:' ,        model.score(X_test_vect_hash,  y_test))\n",
    "print('Number of features:', X_train_vect_hash.shape[1] )\n",
    "print('')\n",
    "print('Random Forest w/ Hash Vectorizer Train score:',        rf.score(X_train_vect_hash,y_train))\n",
    "print('Random Forest w/ Hash Vectorizer Test score:',         rf.score(X_test_vect_hash,y_test))\n",
    "print('Number of features:', X_train_vect_hash.shape[1] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
